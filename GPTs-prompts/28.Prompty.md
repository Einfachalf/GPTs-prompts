A Complete Introduction to Prompt Engineering For Large Language Models  
October 2022  
  
In recent years with the release of large language models (LLMs) pretrained on massive text corpora, a new paradigm for building natural language processing systems has emerged.  
  
Rather than the conventional methodology of building text applications that has been used for decades and relies on a carefully curated labelled training set, LLMs have birthed a new technique called .  
  
In the prompting paradigm, a pretrained LLM is provided a snippet of text as an input and is expected to provide a relevant completion of this input. .ese inputs may describe a task being asked of the model such as:  
  
Translate the following sentence from English to Spanish.  
  
The cat jumped over the moon.  
  
and the model is expected to return:  
  
El gato saltó por encima de la luna.  
  
.e extraordinary thing about prompting is that if these inputs are appropriately cra.ted, a single LLM can be adapted to scores of diverse tasks such as summarization, question answering, SQL generation, and translation with a handful (or zero) training samples.  
  
Because the performance of these LLMs is so dependent on the inputs fed into them, researchers and industry practitioners have developed the discipline of which is intended to provide a set of principles and techniques for designing prompts to squeeze out the best performance from these machine learning juggernauts.  
  
For this reason, prompt engineering is also sometimes called prompt programming or even natural language programming.  
  
In this post, I will provide a comprehensive review of the most interesting research techniques and use-cases in prompt engineering as applied to large language models. My goal is also to provide a set of actionable steps for being a more e.fective prompt engineer.  
  
If you’re looking for a TLDR, here’s a cheatsheet with tips/tricks when designing LLM prompts:  
  
Otherwise, let’s begin.
